{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ca1a1904-bde5-4068-92db-5b35c314e18b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Aconex Workflow Extraction\n",
    "\n",
    "### Summary\n",
    "- This notebook uses Aconex API to extract all ONxpress workflows for analytics & reporting purposes.\n",
    "\n",
    "### Input\n",
    "> _No external input required._\n",
    "\n",
    "### API\n",
    "- API Vendor: Oracle Aconex\n",
    "- Authentication: Basic (API key and credentials)\n",
    "\n",
    "### The Execution Flow\n",
    "1. Find out the number of pages.\n",
    "2. Loop through the pages while storing the data on each page in a dataframe.\n",
    "3. Convert the dataframe to spark dataframe.\n",
    "4. Update the existing data in Delta table with new data.\n",
    "\n",
    "### Output\n",
    "- `dev.dept.aconex_workflows`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "71eed3fe-2906-4c72-8a42-c9b06f786cc4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Installs & Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "75071c88-f566-460c-aeee-4aab1919f30e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%pip install xmltodict\n",
    "%pip install tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2b480301-384c-43d8-a8c1-e3cb85be554b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from requests.auth import HTTPBasicAuth\n",
    "from cryptography.fernet import Fernet\n",
    "from tqdm.notebook import trange, tqdm\n",
    "\n",
    "import xmltodict, keyring, requests\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import time\n",
    "import ast\n",
    "\n",
    "from delta.tables import DeltaTable\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import StructType, StructField, StringType"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "59f43bb5-eea0-464b-9501-c8ac60a0419e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Global Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6df33edb-1824-4009-a5b4-56f518684419",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Detla table path\n",
    "table_name_in_catalog = \"dev.dept.aconex_workflows\"\n",
    "\n",
    "# Credentials\n",
    "KEY = \"KEY\"\n",
    "TOKEN = \"TOKEN\"\n",
    "API_KEY = \"API KEY\"\n",
    "\n",
    "# Delta table schema\n",
    "SCHEMA = StructType([\n",
    "\t\tStructField(\"column1\", StringType(), True),\n",
    "\t\tStructField(\"column2\", StringType(), True),\n",
    "\t\tStructField(\"column3\", StringType(), True),\n",
    "\t\tStructField(\"column4\", StringType(), True),\n",
    "\t\tStructField(\"column5\", StringType(), True),\n",
    "\t\tStructField(\"column6\", StringType(), True),\n",
    "\t\tStructField(\"column7\", StringType(), True),\n",
    "\t\tStructField(\"column8\", StringType(), True),\n",
    "\t\tStructField(\"column9\", StringType(), True),\n",
    "\t\tStructField(\"column10\", StringType(), True),\n",
    "\t\tStructField(\"column11\", StringType(), True),\n",
    "\t\tStructField(\"column12\", StringType(), True),\n",
    "\t\tStructField(\"column13\", StringType(), True),\n",
    "\t\tStructField(\"column14\", StringType(), True),\n",
    "\t\tStructField(\"column15\", StringType(), True),\n",
    "\t\tStructField(\"column16\", StringType(), True),\n",
    "\t\tStructField(\"column17\", StringType(), True),\n",
    "\t\tStructField(\"column18\", StringType(), True)\n",
    "    ])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "146162ea-2f33-45b4-99ee-cdd63ffbec78",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Function Definitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "bfa1f6fb-b721-4f2f-9b53-c8380d633b7d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Functon to fetch a single page of the Workflow module data from Aconex\n",
    "def get_response(page_number = 1, max_page_size = 500):\n",
    "\n",
    "    url = \"https://ca1.aconex.com/api/projects/1879053393/workflows\"\n",
    "    auth = HTTPBasicAuth(\"mpatel\", Fernet(KEY).decrypt(TOKEN).decode())\n",
    "\n",
    "    headers = {\n",
    "        \"X-Application-Key\": API_KEY\n",
    "    }\n",
    "\n",
    "    params = {\n",
    "        \"page_size\": max_page_size,\n",
    "        \"page_number\": page_number,\n",
    "        #updated_after: \n",
    "    }\n",
    "\n",
    "    response_raw = requests.get(url, headers=headers, auth=auth, params=params)\n",
    "    response_parsed = xmltodict.parse(response_raw.text.replace(\"\\x02\",\" \"), encoding='utf-8')\n",
    "    \n",
    "    return response_parsed, response_raw.status_code\n",
    "\n",
    "\n",
    "# Function to split a dataframe into list of dataframe batches\n",
    "def df_batcher(df, batch_size):\n",
    "    batches = []\n",
    "    total_batches = len(df) // batch_size + (1 if len(df) % batch_size != 0 else 0)\n",
    "    for i in range(total_batches):\n",
    "        start_index = i * batch_size\n",
    "        end_index = start_index + batch_size\n",
    "        batches.append(df[start_index:end_index])\n",
    "    return batches\n",
    "\n",
    "\n",
    "# Function to parse through the assignees dictionary and return a string of concatenated assignee names\n",
    "def parse_assignees(dict_value):\n",
    "\n",
    "    # If dictionary, there is only one assignee\n",
    "    if type(dict_value['Assignee']) == dict:\n",
    "\n",
    "        # Concat Assignee Name and Organization Name\n",
    "        return dict_value['Assignee']['Name'] + \" - \" + dict_value['Assignee']['OrganizationName']\n",
    "    \n",
    "    # If list of dictionaries, there are multiple assignees\n",
    "    elif type(dict_value['Assignee']) == list:\n",
    "\n",
    "        # Concat each Assignee Name and Organization Name, and join with comma\n",
    "        return ', '.join([f\"{i['Name']} - {i['OrganizationName']}\" for i in dict_value['Assignee']])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e77ed1fe-4bfb-4eb7-aafb-b4c3f3319df2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ec64e144-3531-45c1-b4fe-4bdc1839672a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#### Initializations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "65dd3d17-0c97-459b-a331-5f2326541f2e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Get total number of pages for the 'for' loop in next code block\n",
    "\n",
    "response, status_code = get_response()\n",
    "\n",
    "if status_code != 200:\n",
    "    print(status_code)\n",
    "    print(response)\n",
    "else:\n",
    "    total_pages = int(response['WorkflowSearch']['@TotalPages'])\n",
    "    print(f\"Total Pages: {total_pages}\\nTotal Items: {response['WorkflowSearch']['@TotalResults']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "bd2b66ee-84b9-41ee-a9a2-4a3688e29983",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#### Fetch Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6a678169-c7e4-40ba-89f3-a789dd0bfef7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Define an empty dataframe to append the results to\n",
    "df = pd.DataFrame(columns=['column1','column2','column3','column4','column5','column6','column7','column8','column9','column10','column11','column12','column13','column14','column15','column16','column17','column18'])\n",
    "\n",
    "# Parse through each page of the Workflows module and append data to DataFrame\n",
    "for i in trange(1,total_pages+1):\n",
    "    response, status_code = get_response(page_number=i)\n",
    "    while status_code !=200:\n",
    "        time.sleep(5)\n",
    "        response, status_code = get_response(page_number=i)\n",
    "\n",
    "    page_content = response['WorkflowSearch']['SearchResults']['Workflow']\n",
    "    temp_df = pd.DataFrame(response['WorkflowSearch']['SearchResults']['Workflow'])\n",
    "    df = pd.concat([df,temp_df],ignore_index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "10d4afb7-4442-44e0-8d3c-4fc3ee482f43",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#### Data Prep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "354d7629-ac5c-48db-ba87-309d26521ce0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Remove unnencessary columns\n",
    "df = df[['column1','column2','column3','column4','column5','column6','column7','column8','column9','column10','column11','column12','column13','column14','column15','column16','column17','column18']]\n",
    "\n",
    "# Create column with parsed assignees\n",
    "df['AssignedTo'] = df['Assignees'].apply(parse_assignees)\n",
    "\n",
    "# Remove old Assignees column\n",
    "df = df.drop('Assignees', axis=1)\n",
    "\n",
    "# Convert data type all values of the dataframe to string\n",
    "df = df.astype(str)\n",
    "\n",
    "# Rename columns of the dataframe\n",
    "df.rename(\n",
    "    columns={\n",
    "        \"old_column\":\"NewColumn\"\n",
    "    },\n",
    "    inplace=True\n",
    ")\n",
    "\n",
    "# Remove true duplicate rows\n",
    "#df.drop_duplicates(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a52dba22-4989-48d2-9f14-d5fb3eb9a3e9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Check for duplicate Document IDs\n",
    "len(df) == len(set(df[\"WorkflowID\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "74eb1658-bbb7-4a6e-8ae3-c67d103792d2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1f53cda1-e18b-4095-bfc0-fd29526c4b19",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#### Data Save"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "99be7dfd-fdf1-4fab-b159-50463670076f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Split the dataframe into list of batches\n",
    "df_batches = df_batcher(df, 25000)\n",
    "\n",
    "# Check breakdown of batches\n",
    "[len(i) for i in df_batches]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "87e12efa-cda8-4563-bf3c-bc4c75137dfb",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Save batches to delta table in a for loop\n",
    "count = 0\n",
    "for t in trange(len(df_batches)):\n",
    "    spark_df = spark.createDataFrame(df_batches[t], schema=SCHEMA)\n",
    "    if count == 0:\n",
    "        # Overwrite data for the first batch\n",
    "        spark_df.write.format(\"delta\").option(\"delta.columnMapping.mode\", \"name\").option(\"mergeSchema\", \"true\").mode(\"overwrite\").saveAsTable(table_name_in_catalog)\n",
    "    else:\n",
    "        # Append data for the rest of the batches\n",
    "        spark_df.write.format(\"delta\").option(\"delta.columnMapping.mode\", \"name\").option(\"mergeSchema\", \"true\").mode(\"append\").saveAsTable(table_name_in_catalog)\n",
    "    print(count)\n",
    "    count += 1"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "1"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 2476617071933374,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "Aconex Workflow Extraction",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
